{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3260763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimized value of A12, A21 = [1.9580714 1.6893563]\n",
      " value of loss function = 0.6702041\n"
     ]
    }
   ],
   "source": [
    "#### PROBLEM 1 Solution \n",
    "\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Measured Data for x1 and p \n",
    "x1_data = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p_data = [28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5]\n",
    "\n",
    "# saturation pressure = 10.0**((a[1] - (a[2] / (T + a[3]))))\n",
    "psat_wat = 10.0**(8.07131-1730.63/(20.0+233.426))\n",
    "psat_diox = 10.0**(7.43155-1554.679/(20.0+240.337))\n",
    "\n",
    "#defining a loss func.\n",
    "def loss(a): \n",
    "    loss_value = 0.0\n",
    "    for i in range(11):\n",
    "        x1 = x1_data[i]\n",
    "        p = p_data[i]\n",
    "        p_fit = x1*psat_wat*t.exp(a[0]*(a[1]*(1-x1)/(a[0]*x1+a[1]*(1-x1)))**2) + (1-x1)*psat_diox*t.exp(a[1]*( a[0]*x1/( a[0]*x1+a[1]*(1-x1) ) )**2)\n",
    "        loss_value = loss_value + (p_fit-p)**2\n",
    "    return loss_value\n",
    "#assume error as 1\n",
    "error = 1\n",
    "#initialize A12 and A21 \n",
    "A = Variable(t.tensor([1.0, 1.0]), requires_grad=True)\n",
    "\n",
    "# gradient decent\n",
    "while error >= 0.1:\n",
    "    loss(A).backward()\n",
    "    error = t.linalg.norm(A.grad)\n",
    "    # Perform line search n ensure smaller step size\n",
    "    s=.1  # intialize step size\n",
    "    while loss(A-s*A.grad) > loss(A):\n",
    "        s=.3*s\n",
    "    with t.no_grad():       \n",
    "        A -= s*A.grad\n",
    "        A.grad.zero_()\n",
    "\n",
    "print(' Optimized value of A12, A21 =', A.data.numpy())\n",
    "print(' value of loss function =', loss(A).data.numpy())\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48effec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46d224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/prashanth/opt/anaconda3/lib/python3.8/site-packages (1.9.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/prashanth/opt/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "divine-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -4.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Define a loss\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-starter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -6.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "x.data = t.tensor([2.0, 1.0])\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss.backward()\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infectious-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.        1.9999971]\n",
      "8.185452e-12\n"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.01\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(1000):  # TODO: change the termination criterion\n",
    "    loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eb0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be190f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
